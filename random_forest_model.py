# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oQZZvCM7T0-Ev_0p68kXg7Nb3a4UVCr4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from google.colab import files

# Load the dataset
try:
    df = pd.read_csv("Life Expectancy Data.csv")
except FileNotFoundError:
    print("Error: Could not find 'Life Expectancy Data.csv' in the current directory.")
    print("Please make sure the data file is in the same directory as this script.")
    exit(1)

df.head() # ilk 5 satÄ±rÄ± gÃ¶ster

print("Veri setindeki tÃ¼m sÃ¼tunlar:")
for i, col in enumerate(df.columns):
    print(f"{i+1}. {col}")

#sayÄ±sal ve kategorik sÄ±nÄ±flarÄ± ayrÄ±ÅŸtÄ±rma
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print("SayÄ±sal (numerik) sÃ¼tunlar:", numeric_cols)
print("\nKategorik sÃ¼tunlar:", categorical_cols)

#sayÄ±sal Ã¶zelliklerin daÄŸÄ±lÄ±mÄ±
df[numeric_cols].hist(bins=20, figsize=(15, 12), color='skyblue', edgecolor='black')
plt.suptitle("SayÄ±sal Ã–zelliklerin DaÄŸÄ±lÄ±mÄ±", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Kategorik (object tipinde) tÃ¼m sÃ¼tunlarÄ± listele
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print("Kategorik sÃ¼tunlar:", categorical_cols)

for col in categorical_cols:
    print(f"\nğŸ“Œ {col} â†’ unique deÄŸerler:")
    print(df[col].unique())

# 'Status' gibi binary (2 sÄ±nÄ±flÄ±) kategorik sÃ¼tunlar iÃ§in One-Hot Encoding
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

missing = df.isnull().mean() * 100
missing = missing[missing > 0].sort_values()

plt.figure(figsize=(10, 8))
missing.plot(kind='barh', color='salmon')
plt.title("Eksik Veri OranÄ± (%)")
plt.xlabel("YÃ¼zde")
plt.ylabel("Ã–zellikler")
plt.grid(True, axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# veri tÃ¼rlerinin daÄŸÄ±lÄ±mÄ±
type_counts = df.dtypes.value_counts()

plt.figure(figsize=(6,6))
plt.pie(type_counts, labels=type_counts.index.astype(str), autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])
plt.title("Veri TÃ¼rlerine GÃ¶re Ã–zellik DaÄŸÄ±lÄ±mÄ±")
plt.axis('equal')
plt.show()

# Orijinal verinin bir kopyasÄ±nÄ± al (temizlenmemiÅŸ haliyle)
raw_df = pd.read_csv("Life Expectancy Data.csv") #temizlik Ã¶ncesi verinin kopyasÄ±

# 2ï¸âƒ£ SayÄ±sal sÃ¼tunlardaki eksik verileri ortalama ile dolduruyoruz
df = df.fillna(df.mean(numeric_only=True))

summary_df = pd.DataFrame({
    "AÃ§Ä±klama": ["Toplam SatÄ±r", "Toplam SÃ¼tun", "Toplam Eksik HÃ¼cre", "Eksik Veri OranÄ± (%)"],
    "Temizlik Ã–ncesi": [
        raw_df.shape[0],
        raw_df.shape[1],
        raw_df.isnull().sum().sum(),
        round((raw_df.isnull().sum().sum() / raw_df.size) * 100, 2)
    ],
    "Temizlik SonrasÄ±": [
        df.shape[0],
        df.shape[1],
        df.isnull().sum().sum(),
        round((df.isnull().sum().sum() / df.size) * 100, 2)
    ]
})

print(summary_df)

total_missing_raw = raw_df.isnull().sum().sum()
total_missing_clean = df.isnull().sum().sum()

plt.bar(["Temizlik Ã–ncesi", "Temizlik SonrasÄ±"], [total_missing_raw, total_missing_clean], color=["orange", "green"])
plt.ylabel("Eksik HÃ¼cre SayÄ±sÄ±")
plt.title("Toplam Eksik HÃ¼cre SayÄ±sÄ± (KarÅŸÄ±laÅŸtÄ±rmalÄ±)")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

# 1ï¸âƒ£ Hedef (baÄŸÄ±mlÄ±) deÄŸiÅŸkeni ayÄ±rÄ±yoruz: 'Life expectancy '
# Modelin tahmin edeceÄŸi ÅŸey bu
y = df["Life expectancy "]

# 2ï¸âƒ£ Girdi (baÄŸÄ±msÄ±z) deÄŸiÅŸkenleri ayÄ±rÄ±yoruz: Geri kalan tÃ¼m sÃ¼tunlar
# Modelin Ã¶ÄŸrenmesini saÄŸlayacak Ã¶zellikler
X = df.drop(columns=["Life expectancy "])

# 3ï¸âƒ£ EÄŸitim ve test verilerine ayÄ±rÄ±yoruz
# EÄŸitim verisi modelin Ã¶ÄŸrenmesi iÃ§in (%80)
# Test verisi modelin doÄŸruluÄŸunu Ã¶lÃ§mek iÃ§in (%20)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,              # Girdiler ve hedef
    test_size=0.2,     # %20 test verisi
    random_state=42    # AynÄ± sonuÃ§larÄ± almak iÃ§in sabit tohum deÄŸeri
)

# 1ï¸âƒ£ Lineer Regresyon Modeli oluÅŸturma
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# 2ï¸âƒ£ Test verisi Ã¼zerinde tahmin yap
y_pred_lr = lr_model.predict(X_test)

# 3ï¸âƒ£ Metrikleri hesapla
mae_lr = mean_absolute_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)

print("ğŸ”¹ Linear Regression SonuÃ§larÄ±:")
print(f"MAE  : {mae_lr:.2f}")
print(f"RMSE : {rmse_lr:.2f}")
print(f"RÂ²   : {r2_lr:.2f}")

# 4ï¸âƒ£ GerÃ§ek vs Tahmin GrafiÄŸi
plt.figure(figsize=(6,5))
plt.scatter(y_test, y_pred_lr, alpha=0.6, color='royalblue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("GerÃ§ek YaÅŸam SÃ¼resi")
plt.ylabel("Tahmin Edilen YaÅŸam SÃ¼resi")
plt.title("Linear Regression - GerÃ§ek vs Tahmin")
plt.grid(True)
plt.tight_layout()
plt.show()

# 5ï¸âƒ£ ArtÄ±k (residual) daÄŸÄ±lÄ±mÄ±
residuals_lr = y_test - y_pred_lr
plt.figure(figsize=(6,4))
plt.hist(residuals_lr, bins=20, color='darkorange', edgecolor='black')
plt.title("Linear Regression - ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ±")
plt.xlabel("ArtÄ±k (GerÃ§ek - Tahmin)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Ne SonuÃ§ Ã‡Ä±karabiliriz?
#Lineer regresyon ortalama tahminlerde iyi iÅŸ Ã§Ä±karÄ±yor.

#Ancak bazÄ± noktalar iÃ§in doÄŸrusal iliÅŸki yetersiz kalÄ±yor olabilir.

#Bu durumda:

#Non-lineer modeller (Decision Tree, Random Forest) denemek mantÄ±klÄ± olur.

#AyrÄ±ca uÃ§ deÄŸerleri gÃ¶zlemleyip filtrelemek model kalitesini artÄ±rabilir.

# 1ï¸âƒ£ Decision Modeli oluÅŸtur ve eÄŸit
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# 2ï¸âƒ£ Tahmin yap
y_pred_dt = dt_model.predict(X_test)

# 3ï¸âƒ£ Metrikleri hesapla
mae_dt = mean_absolute_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))
r2_dt = r2_score(y_test, y_pred_dt)

# 4ï¸âƒ£ SonuÃ§larÄ± yazdÄ±r
print("ğŸŒ³ Decision Tree Regressor SonuÃ§larÄ±:")
print(f"MAE  : {mae_dt:.2f}")
print(f"RMSE : {rmse_dt:.2f}")
print(f"RÂ²   : {r2_dt:.2f}")

# 5ï¸âƒ£ GerÃ§ek vs Tahmin GrafiÄŸi
plt.figure(figsize=(6,5))
plt.scatter(y_test, y_pred_dt, alpha=0.6, color='seagreen')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("GerÃ§ek YaÅŸam SÃ¼resi")
plt.ylabel("Tahmin Edilen YaÅŸam SÃ¼resi")
plt.title("Decision Tree - GerÃ§ek vs Tahmin")
plt.grid(True)
plt.tight_layout()
plt.show()

# 6ï¸âƒ£ ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ±
residuals_dt = y_test - y_pred_dt
plt.figure(figsize=(6,4))
plt.hist(residuals_dt, bins=20, color='forestgreen', edgecolor='black')
plt.title("Decision Tree - ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ±")
plt.xlabel("ArtÄ±k (GerÃ§ek - Tahmin)")
plt.grid(True)
plt.tight_layout()
plt.show()

# 1ï¸âƒ£ Random forest Modeli oluÅŸtur ve eÄŸit
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 2ï¸âƒ£ Tahmin yap
y_pred_rf = rf_model.predict(X_test)

# 3ï¸âƒ£ Metrikleri hesapla
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

# 4ï¸âƒ£ SonuÃ§larÄ± yazdÄ±r
print("ğŸŒ² Random Forest Regressor SonuÃ§larÄ±:")
print(f"MAE  : {mae_rf:.2f}")
print(f"RMSE : {rmse_rf:.2f}")
print(f"RÂ²   : {r2_rf:.2f}")

# 5ï¸âƒ£ GerÃ§ek vs Tahmin GrafiÄŸi
plt.figure(figsize=(6,5))
plt.scatter(y_test, y_pred_rf, alpha=0.6, color='darkblue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("GerÃ§ek YaÅŸam SÃ¼resi")
plt.ylabel("Tahmin Edilen YaÅŸam SÃ¼resi")
plt.title("Random Forest - GerÃ§ek vs Tahmin")
plt.grid(True)
plt.tight_layout()
plt.show()

# 6ï¸âƒ£ ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ±
residuals_rf = y_test - y_pred_rf
plt.figure(figsize=(6,4))
plt.hist(residuals_rf, bins=20, color='steelblue', edgecolor='black')
plt.title("Random Forest - ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ±")
plt.xlabel("ArtÄ±k (GerÃ§ek - Tahmin)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Daha Ã¶nce elde edilen sonuÃ§larÄ± manuel olarak giriyoruz
comparison_df = pd.DataFrame({
    "Model": ["Linear Regression", "Decision Tree", "Random Forest"],
    "MAE": [2.86, 1.55, 1.05],
    "RMSE": [3.90, 2.55, 1.64],
    "RÂ²": [0.82, 0.93, 0.97]
})

# Tabloyu yazdÄ±r
print("ğŸ“Š ÃœÃ§ Modelin Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±")
display(comparison_df)

# TÃ¼m grafiklerin yan yana gÃ¶sterileceÄŸi 2 satÄ±r Ã— 3 sÃ¼tunluk grid yapÄ±sÄ±
fig, axs = plt.subplots(2, 3, figsize=(18, 10))

# ============================
# SatÄ±r 1: GerÃ§ek vs Tahmin
# ============================
axs[0, 0].scatter(y_test, y_pred_lr, alpha=0.6, color='royalblue')
axs[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
axs[0, 0].set_title("Linear Regression - GerÃ§ek vs Tahmin")
axs[0, 0].set_xlabel("GerÃ§ek")
axs[0, 0].set_ylabel("Tahmin")
axs[0, 0].grid(True)

axs[0, 1].scatter(y_test, y_pred_dt, alpha=0.6, color='seagreen')
axs[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
axs[0, 1].set_title("Decision Tree - GerÃ§ek vs Tahmin")
axs[0, 1].set_xlabel("GerÃ§ek")
axs[0, 1].grid(True)

axs[0, 2].scatter(y_test, y_pred_rf, alpha=0.6, color='darkorange')
axs[0, 2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
axs[0, 2].set_title("Random Forest - GerÃ§ek vs Tahmin")
axs[0, 2].set_xlabel("GerÃ§ek")
axs[0, 2].grid(True)

# ============================
# SatÄ±r 2: ArtÄ±k DaÄŸÄ±lÄ±mÄ±
# ============================
axs[1, 0].hist(y_test - y_pred_lr, bins=20, color='royalblue', edgecolor='black')
axs[1, 0].set_title("Linear Regression - ArtÄ±klar")
axs[1, 0].set_xlabel("ArtÄ±k")
axs[1, 0].set_ylabel("Frekans")
axs[1, 0].grid(True)

axs[1, 1].hist(y_test - y_pred_dt, bins=20, color='seagreen', edgecolor='black')
axs[1, 1].set_title("Decision Tree - ArtÄ±klar")
axs[1, 1].set_xlabel("ArtÄ±k")
axs[1, 1].grid(True)

axs[1, 2].hist(y_test - y_pred_rf, bins=20, color='darkorange', edgecolor='black')
axs[1, 2].set_title("Random Forest - ArtÄ±klar")
axs[1, 2].set_xlabel("ArtÄ±k")
axs[1, 2].grid(True)

plt.suptitle("Model PerformansÄ±: GerÃ§ek vs Tahmin ve ArtÄ±klarÄ±n KarÅŸÄ±laÅŸtÄ±rmasÄ±", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pandas as pd
import numpy as np

# 1ï¸âƒ£ EÄŸitim ve test tahminleri
y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# 2ï¸âƒ£ Metrikler
r2_train = r2_score(y_train, y_pred_train)
mae_train = mean_absolute_error(y_train, y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))

r2_test = r2_score(y_test, y_pred_test)
mae_test = mean_absolute_error(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

# 3ï¸âƒ£ KarÅŸÄ±laÅŸtÄ±rmalÄ± tablo
metrics_df = pd.DataFrame({
    "Veri Seti": ["EÄŸitim", "Test"],
    "RÂ²": [r2_train, r2_test],
    "MAE": [mae_train, mae_test],
    "RMSE": [rmse_train, rmse_test]
})

# 4ï¸âƒ£ Otomatik yorumlayÄ±cÄ±
print("ğŸ“Š Overfitting/Underfitting KarÅŸÄ±laÅŸtÄ±rmasÄ±\n")
display(metrics_df)

print("\nğŸ” Yorumlama:")

r2_gap = r2_train - r2_test

if r2_gap > 0.1 and r2_test < 0.9:
    print("âš ï¸ Overfitting tespit edildi: EÄŸitim RÂ² Ã§ok yÃ¼ksek, test RÂ² dÃ¼ÅŸÃ¼k.")
elif r2_train < 0.8 and r2_test < 0.8:
    print("ğŸ“‰ Underfitting tespit edildi: Model hem eÄŸitim hem test verisinde baÅŸarÄ±sÄ±z.")
elif abs(r2_gap) <= 0.1 and r2_test >= 0.9:
    print("âœ… Model genelleme baÅŸarÄ±sÄ± yÃ¼ksek. Overfitting / underfitting gÃ¶rÃ¼nmÃ¼yor.")
else:
    print("â„¹ï¸ Model dengeli olabilir, ancak skorlar incelenmeli.")

#Cross Validation hesaplamasÄ±

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer

# RÂ² iÃ§in 5-fold cross-validation
cv_scores_r2 = cross_val_score(rf_model, X, y, cv=5, scoring='r2')

# MAE iÃ§in 5-fold cross-validation (negatif dÃ¶ner, pozitif yapmak iÃ§in ters Ã§evrilir)
mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)
cv_scores_mae = cross_val_score(rf_model, X, y, cv=5, scoring=mae_scorer)

# Ortalama skorlarÄ± yazdÄ±r
print("ğŸ“Š Cross-Validation SkorlarÄ± (5-Fold):")
print("RÂ² SkorlarÄ±:", np.round(cv_scores_r2, 4))
print("Ortalama RÂ²:", np.mean(cv_scores_r2).round(4))
print("\nMAE SkorlarÄ±:", np.round(-cv_scores_mae, 4))
print("Ortalama MAE:", (-np.mean(cv_scores_mae)).round(4))

mean_r2 = np.mean(cv_scores_r2)
mean_mae = -np.mean(cv_scores_mae)

# 7ï¸âƒ£ KarÅŸÄ±laÅŸtÄ±rma tablosu
comparison_df = pd.DataFrame({
    "Metrik": ["RÂ²", "MAE"],
    "Cross-Validation (Ortalama)": [mean_r2, mean_mae],
    "Test Skoru": [r2_test, mae_test]
})

# 8ï¸âƒ£ YazdÄ±r
print("ğŸ“Š Cross-Validation vs Test Skoru KarÅŸÄ±laÅŸtÄ±rmasÄ±")
print(comparison_df)

# FarkÄ±n yÃ¼zde cinsinden hesaplanmasÄ±
r2_gap_percent = abs(r2_test - mean_r2) / r2_test * 100

print(f"ğŸ“ Test ve Cross-Validation RÂ² farkÄ±: {r2_gap_percent:.2f}%")

# Yorumlama
print("ğŸ” Yorumlama:")
if r2_gap_percent <= 5:
    print("âœ… Model genelleme konusunda Ã§ok baÅŸarÄ±lÄ±. Ezberleme (overfitting) yapmÄ±yor.")
elif r2_gap_percent <= 10:
    print("ğŸŸ¡ Model genellemesi kabul edilebilir seviyede. KÃ¼Ã§Ã¼k fark doÄŸal.")
else:
    print("â— Model yÃ¼ksek ihtimalle overfitting yapÄ±yor. FarklÄ± veri setlerinde performansÄ± dÃ¼ÅŸebilir.")

#En uygun model olarak random forest modeli seÃ§ildi.
#Cross- Validation ile genelleme bulundu. Belirli bir oranda overfitting olabilceÄŸi gÃ¶zlemlendi.
#Overfitting riskinin azaltÄ±lmasÄ± iÃ§in Random forest modelini geliÅŸtirme aÅŸamalarÄ±na geÃ§ildi.
#Random Forest modelini geliÅŸtirme adÄ±mlarÄ±na geÃ§elim

#Features Ä°mportance deÄŸerlerin bulunmasÄ±

# 1ï¸âƒ£ Ã–zellik isimlerini ve Ã¶nem skorlarÄ±nÄ± al
feature_names = X.columns  # TÃ¼m orijinal Ã¶znitelik isimleri
importances = rf_model.feature_importances_  # Random Forest modeli Ã¼zerinden alÄ±nÄ±r
indices = np.argsort(importances)[::-1]  # BÃ¼yÃ¼kten kÃ¼Ã§Ã¼ÄŸe sÄ±rala

# 2ï¸âƒ£ GÃ¶rsel olarak Ã§iz (ilk 15 Ã¶zellik iÃ§in)
plt.figure(figsize=(12, 6))
plt.title("Random Forest - Ã–zellik Ã–nem Skoru")
plt.bar(range(15), importances[indices[:15]], align="center", color="mediumseagreen")
plt.xticks(range(15), feature_names[indices[:15]], rotation=90)
plt.xlabel("Ã–zellikler")
plt.ylabel("Ã–nem Skoru")
plt.tight_layout()
plt.show()

# 3ï¸âƒ£ Tablo olarak da ilk 10 Ã¶zelliÄŸi gÃ¶ster (opsiyonel)
top_features_df = pd.DataFrame({
    "Ã–zellik": feature_names[indices],
    "Ã–nem Skoru": importances[indices]
}).head(10)

top_features_df

# EÄŸitim verisinde tahmin yap
y_pred_train = rf_model.predict(X_train)

# EÄŸitim metrikleri
r2_train = r2_score(y_train, y_pred_train)
mae_train = mean_absolute_error(y_train, y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
print(f"R2 deÄŸeri: {r2_train}")
print(f"MAE deÄŸeri: {mae_train}")
print(f"RMSE deÄŸeri: {rmse_train}")

#Hiperparametre optimizasyonu

from sklearn.model_selection import GridSearchCV

# Hiperparametre aralÄ±ÄŸÄ± (hatalÄ± 'auto' kaldÄ±rÄ±ldÄ±)
param_grid = {
    'n_estimators': [100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'max_features': ['sqrt', 'log2']
}

grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_
y_pred_best = best_rf.predict(X_test)

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

r2_best = r2_score(y_test, y_pred_best)
mae_best = mean_absolute_error(y_test, y_pred_best)
rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))  # dÃ¼zeltildi

print("ğŸ”§ En iyi hiperparametreler:")
print(grid_search.best_params_)

print("\nâœ… Optimize edilmiÅŸ modelin test skorlarÄ±:")
print(f"RÂ²: {r2_best:.3f}")
print(f"MAE: {mae_best:.3f}")
print(f"RMSE: {rmse_best:.3f}")

# ğŸ” Optimize edilmiÅŸ model iÃ§in cross-validation testleri
cv_scores_r2_opt = cross_val_score(best_rf, X, y, cv=5, scoring='r2')
mean_r2_opt = np.mean(cv_scores_r2_opt)

cv_scores_mae_opt = cross_val_score(best_rf, X, y, cv=5, scoring=make_scorer(mean_absolute_error, greater_is_better=False))
mean_mae_opt = -np.mean(cv_scores_mae_opt)

# ğŸ“‹ Performans karÅŸÄ±laÅŸtÄ±rma tablosu
comparison_opt_df = pd.DataFrame({
    "Metrik": ["RÂ²", "MAE"],
    "Cross-Validation (Ortalama)": [mean_r2_opt, mean_mae_opt],
    "Test Skoru": [r2_best, mae_best]
})

print("ğŸ“Š Optimize EdilmiÅŸ Random Forest Model PerformansÄ±:")
print(comparison_opt_df)

# ğŸ“ RÂ² farkÄ±nÄ±n yÃ¼zdesel olarak hesaplanmasÄ±
r2_gap_percent_opt = abs(r2_best - mean_r2_opt) / r2_best * 100
print(f"\nğŸ“ Test ve CV RÂ² farkÄ±: {r2_gap_percent_opt:.2f}%")

# ğŸ” Yorumlama
print("ğŸ” Yorumlama:")
if r2_gap_percent_opt <= 5:
    print("âœ… Model genelleme konusunda Ã§ok baÅŸarÄ±lÄ±. Overfitting gÃ¶zlemlenmiyor.")
elif r2_gap_percent_opt <= 10:
    print("ğŸŸ¡ Model kabul edilebilir dÃ¼zeyde. KÃ¼Ã§Ã¼k bir fark doÄŸal.")
else:
    print("â— Overfitting riski olabilir. Test ve CV skorlarÄ± arasÄ±nda anlamlÄ± fark var.")

#hiperparametre aralÄ±ÄŸÄ±nÄ±n daraltÄ±larak modeli kabul edilebilir dÃ¼zeyden Genelleme konusunda baÅŸarÄ±lÄ± hale dÃ¶nÃ¼ÅŸtÃ¼rmek.

# Gerekli kÃ¼tÃ¼phaneler
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_absolute_error, make_scorer

# 1ï¸âƒ£ Veriyi yÃ¼kle
df = pd.read_csv("Life Expectancy Data.csv")  # kendi yoluna gÃ¶re deÄŸiÅŸtir
df = df.drop(columns=["Country"])
df = df.fillna(df.mean(numeric_only=True))
df = pd.get_dummies(df, columns=["Status"], drop_first=True)

# 2ï¸âƒ£ Ã–zellik ve hedef ayrÄ±mÄ±
X = df.drop(columns=["Life expectancy "])
y = df["Life expectancy "]

# 3ï¸âƒ£ EÄŸitim/test bÃ¶lme
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4ï¸âƒ£ DaraltÄ±lmÄ±ÅŸ hiperparametre aralÄ±ÄŸÄ±
param_grid_narrow = {
    'n_estimators': [100, 120],
    'max_depth': [10, 15],
    'min_samples_split': [2, 4],
    'max_features': ['sqrt']
}

# 5ï¸âƒ£ GridSearchCV tanÄ±mÄ±
grid_search_narrow = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid_narrow,
    cv=5,
    scoring='r2',
    verbose=1,
    n_jobs=-1
)

# 6ï¸âƒ£ Modeli eÄŸit
grid_search_narrow.fit(X_train, y_train)
best_rf_narrow = grid_search_narrow.best_estimator_

# 7ï¸âƒ£ Test skorlarÄ±nÄ± hesapla
y_pred_narrow = best_rf_narrow.predict(X_test)
r2_test = r2_score(y_test, y_pred_narrow)
mae_test = mean_absolute_error(y_test, y_pred_narrow)

# 8ï¸âƒ£ Cross-validation skorlarÄ±nÄ± hesapla
cv_scores_r2 = cross_val_score(best_rf_narrow, X, y, cv=5, scoring='r2')
mean_r2 = np.mean(cv_scores_r2)

cv_scores_mae = cross_val_score(
    best_rf_narrow, X, y, cv=5, scoring=make_scorer(mean_absolute_error, greater_is_better=False)
)
mean_mae = -np.mean(cv_scores_mae)

# 9ï¸âƒ£ Fark yÃ¼zdesi
r2_gap_percent = abs(r2_test - mean_r2) / r2_test * 100

# ğŸ”Ÿ Raporlama
print("ğŸ“Š Optimize EdilmiÅŸ Model PerformansÄ±:\n")
print(f"Test RÂ²: {r2_test:.4f}")
print(f"CV Ortalama RÂ²: {mean_r2:.4f}")
print(f"Test MAE: {mae_test:.4f}")
print(f"CV Ortalama MAE: {mean_mae:.4f}")
print(f"RÂ² Test-CV FarkÄ± (%): {r2_gap_percent:.2f}%\n")

# ğŸ“Œ Yorumlama
print("ğŸ” Yorum:")
if r2_gap_percent <= 5:
    print("âœ… Genelleme baÅŸarÄ±lÄ±. Model overfitting yapmÄ±yor.")
elif r2_gap_percent <= 10:
    print("ğŸŸ¡ Kabul edilebilir fark. Dikkatli olunmalÄ±.")
else:
    print("â— Overfitting riski var. CV ve test skoru arasÄ±nda bÃ¼yÃ¼k fark var.")

# Save the model
joblib.dump(best_rf_narrow, 'life_expectancy_rf_model.pkl')

# Save the columns used for training (important for input order)
joblib.dump(list(X.columns), 'model_columns.pkl')